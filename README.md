# NLP_CONCEPTS
# 1.Stopwords 
# 2.Lemmatizer and PorterStemmer
# 3.word2vec
# 4.Bag of words ( CountVectorizer)
# 5.TF-IDF 
# The five steps of data processing is called Tokenization 
After tokenization we can create our machine learning model
# PROJECT ( SPAM CLASSIFIER )
